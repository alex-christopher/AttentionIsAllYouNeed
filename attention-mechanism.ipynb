{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca3d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input is :  torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "batch_size = 4\n",
    "sequence_length = 64\n",
    "embed_dim = 128\n",
    "\n",
    "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "print(\"Shape of input is : \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a9095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26b5276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447a1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (x @ x.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19cd2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5227622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(379.4189)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d56de33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2351e+01, -7.1957e-02,  7.1907e-01,  ...,  4.7201e-01,\n",
       "           2.4169e+00,  4.6512e-01],\n",
       "         [-7.1957e-02,  9.4296e+00, -1.9366e-01,  ...,  5.9690e-01,\n",
       "          -1.0521e-02, -2.3480e-01],\n",
       "         [ 7.1907e-01, -1.9366e-01,  1.2710e+01,  ..., -1.8016e+00,\n",
       "           3.6835e-01,  3.8830e-01],\n",
       "         ...,\n",
       "         [ 4.7201e-01,  5.9690e-01, -1.8016e+00,  ...,  1.1194e+01,\n",
       "           1.9364e+00,  4.6739e-01],\n",
       "         [ 2.4169e+00, -1.0521e-02,  3.6835e-01,  ...,  1.9364e+00,\n",
       "           9.5943e+00,  1.6906e+00],\n",
       "         [ 4.6512e-01, -2.3480e-01,  3.8830e-01,  ...,  4.6739e-01,\n",
       "           1.6906e+00,  9.2651e+00]],\n",
       "\n",
       "        [[ 1.1608e+01,  2.2425e+00,  3.4092e-01,  ..., -1.4978e+00,\n",
       "           1.7040e+00, -1.3147e+00],\n",
       "         [ 2.2425e+00,  1.3480e+01,  3.3930e-01,  ...,  7.6517e-01,\n",
       "          -5.5108e-01, -1.8567e-01],\n",
       "         [ 3.4092e-01,  3.3930e-01,  1.0917e+01,  ...,  2.5809e-01,\n",
       "          -7.4044e-01, -1.4849e-01],\n",
       "         ...,\n",
       "         [-1.4978e+00,  7.6517e-01,  2.5809e-01,  ...,  1.0509e+01,\n",
       "          -3.7496e-02, -2.9299e-01],\n",
       "         [ 1.7040e+00, -5.5108e-01, -7.4044e-01,  ..., -3.7496e-02,\n",
       "           1.0708e+01,  3.1621e-01],\n",
       "         [-1.3147e+00, -1.8567e-01, -1.4849e-01,  ..., -2.9299e-01,\n",
       "           3.1621e-01,  1.1749e+01]],\n",
       "\n",
       "        [[ 9.1639e+00, -1.3398e+00, -2.9515e-01,  ..., -4.2220e-01,\n",
       "           1.3052e+00,  7.5912e-02],\n",
       "         [-1.3398e+00,  1.1138e+01, -7.7700e-02,  ...,  2.0739e-01,\n",
       "           9.9880e-01, -8.2049e-01],\n",
       "         [-2.9515e-01, -7.7700e-02,  1.2004e+01,  ...,  9.0468e-01,\n",
       "          -1.7393e-01,  9.9673e-01],\n",
       "         ...,\n",
       "         [-4.2220e-01,  2.0739e-01,  9.0468e-01,  ...,  8.8990e+00,\n",
       "           2.1255e+00,  5.7775e-01],\n",
       "         [ 1.3052e+00,  9.9880e-01, -1.7393e-01,  ...,  2.1255e+00,\n",
       "           1.4822e+01,  1.6804e+00],\n",
       "         [ 7.5912e-02, -8.2049e-01,  9.9673e-01,  ...,  5.7775e-01,\n",
       "           1.6804e+00,  8.1396e+00]],\n",
       "\n",
       "        [[ 1.1508e+01,  7.1665e-01, -4.8105e-01,  ..., -2.2654e-01,\n",
       "           1.0577e+00,  6.8022e-01],\n",
       "         [ 7.1665e-01,  1.2469e+01, -1.3611e+00,  ..., -2.2818e+00,\n",
       "          -2.3103e-01,  2.4137e-01],\n",
       "         [-4.8105e-01, -1.3611e+00,  1.2030e+01,  ...,  3.0952e+00,\n",
       "           7.8980e-01,  4.1366e-01],\n",
       "         ...,\n",
       "         [-2.2654e-01, -2.2818e+00,  3.0952e+00,  ...,  1.1962e+01,\n",
       "           4.9422e-01,  3.0980e+00],\n",
       "         [ 1.0577e+00, -2.3103e-01,  7.8980e-01,  ...,  4.9422e-01,\n",
       "           9.4531e+00,  1.8213e-01],\n",
       "         [ 6.8022e-01,  2.4137e-01,  4.1366e-01,  ...,  3.0980e+00,\n",
       "           1.8213e-01,  1.3176e+01]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = (x @ x.transpose(1, 2)) / (embed_dim ** 0.5)\n",
    "\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b781a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9956e-01, 4.0245e-06, 8.8767e-06,  ..., 6.9335e-06,\n",
       "          4.8483e-05, 6.8859e-06],\n",
       "         [7.4093e-05, 9.9144e-01, 6.5603e-05,  ..., 1.4463e-04,\n",
       "          7.8787e-05, 6.2958e-05],\n",
       "         [6.2011e-06, 2.4893e-06, 9.9964e-01,  ..., 4.9862e-07,\n",
       "          4.3666e-06, 4.4546e-06],\n",
       "         ...,\n",
       "         [2.2018e-05, 2.4947e-05, 2.2666e-06,  ..., 9.9849e-01,\n",
       "          9.5228e-05, 2.1916e-05],\n",
       "         [7.5734e-04, 6.6849e-05, 9.7641e-05,  ..., 4.6843e-04,\n",
       "          9.9173e-01, 3.6634e-04],\n",
       "         [1.4941e-04, 7.4198e-05, 1.3836e-04,  ..., 1.4975e-04,\n",
       "          5.0885e-04, 9.9114e-01]],\n",
       "\n",
       "        [[9.9882e-01, 8.5511e-05, 1.2769e-05,  ..., 2.0306e-06,\n",
       "          4.9904e-05, 2.4387e-06],\n",
       "         [1.3167e-05, 9.9986e-01, 1.9630e-06,  ..., 3.0052e-06,\n",
       "          8.0581e-07, 1.1613e-06],\n",
       "         [2.5486e-05, 2.5445e-05, 9.9831e-01,  ..., 2.3460e-05,\n",
       "          8.6433e-06, 1.5623e-05],\n",
       "         ...,\n",
       "         [6.0843e-06, 5.8479e-05, 3.5219e-05,  ..., 9.9741e-01,\n",
       "          2.6206e-05, 2.0298e-05],\n",
       "         [1.2258e-04, 1.2854e-05, 1.0637e-05,  ..., 2.1483e-05,\n",
       "          9.9759e-01, 3.0599e-05],\n",
       "         [2.1199e-06, 6.5558e-06, 6.8042e-06,  ..., 5.8887e-06,\n",
       "          1.0829e-05, 9.9914e-01]],\n",
       "\n",
       "        [[9.9091e-01, 2.7187e-05, 7.7275e-05,  ..., 6.8055e-05,\n",
       "          3.8287e-04, 1.1199e-04],\n",
       "         [3.8051e-06, 9.9875e-01, 1.3443e-05,  ..., 1.7877e-05,\n",
       "          3.9446e-05, 6.3958e-06],\n",
       "         [4.5514e-06, 5.6570e-06, 9.9936e-01,  ..., 1.5109e-05,\n",
       "          5.1380e-06, 1.6565e-05],\n",
       "         ...,\n",
       "         [8.8431e-05, 1.6597e-04, 3.3332e-04,  ..., 9.8796e-01,\n",
       "          1.1299e-03, 2.4037e-04],\n",
       "         [1.3483e-06, 9.9250e-07, 3.0720e-07,  ..., 3.0622e-06,\n",
       "          9.9995e-01, 1.9623e-06],\n",
       "         [3.0487e-04, 1.2440e-04, 7.6563e-04,  ..., 5.0357e-04,\n",
       "          1.5169e-03, 9.6854e-01]],\n",
       "\n",
       "        [[9.9867e-01, 2.0557e-05, 6.2060e-06,  ..., 8.0047e-06,\n",
       "          2.8912e-05, 1.9822e-05],\n",
       "         [7.8697e-06, 9.9963e-01, 9.8538e-07,  ..., 3.9243e-07,\n",
       "          3.0506e-06, 4.8927e-06],\n",
       "         [3.6834e-06, 1.5277e-06, 9.9950e-01,  ..., 1.3164e-04,\n",
       "          1.3127e-05, 9.0120e-06],\n",
       "         ...,\n",
       "         [5.0819e-06, 6.5080e-07, 1.4081e-04,  ..., 9.9894e-01,\n",
       "          1.0448e-05, 1.4120e-04],\n",
       "         [2.2382e-04, 6.1691e-05, 1.7122e-04,  ..., 1.2741e-04,\n",
       "          9.9079e-01, 9.3250e-05],\n",
       "         [3.7412e-06, 2.4123e-06, 2.8658e-06,  ..., 4.1979e-05,\n",
       "          2.2735e-06, 9.9961e-01]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mat = similarity.softmax(dim = 2)\n",
    "\n",
    "attn_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a0a33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 64]), torch.Size([4, 64, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mat.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f75062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (attn_mat @ x)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa972806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(10, 20)\n",
    "\n",
    "rand = torch.randn(4, 6, 10)\n",
    "\n",
    "linear(rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314fb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single head attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dimension):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embedding_dimension\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        similarity = (q @ k.transpose(1, 2)) / self.embed_dim ** 0.5\n",
    "        attention = similarity.softmax(axis=2)\n",
    "        output = attention @ v\n",
    "\n",
    "rand = torch.randn(4, 64, 128)\n",
    "\n",
    "attn = Attention(embedding_dimension=128)\n",
    "attn(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b135953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embedding_dimension\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.multihead_qkv = nn.ModuleList()\n",
    "\n",
    "        for head in range(num_heads):\n",
    "            qkv_proj = nn.ModuleDict(\n",
    "                [\n",
    "                  [\"Q\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                  [\"K\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                  [\"V\", nn.Linear(self.embed_dim, self.head_dim)]  \n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.multihead_qkv.append(qkv_proj)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outs = []\n",
    "\n",
    "        for head in self.multihead_qkv:\n",
    "\n",
    "            q = head[\"Q\"](x)\n",
    "            k = head[\"K\"](x)\n",
    "            v = head[\"V\"](x)\n",
    "\n",
    "            similarity = (q @ k.transpose(1, 2)) / self.head_dim ** 0.5\n",
    "            attention = similarity.softmax(axis=-1)\n",
    "            output = attention @ v\n",
    "\n",
    "            head_outs.append(output)\n",
    "\n",
    "        head_outs = torch.cat(head_outs, dim=-1)\n",
    "\n",
    "        out = self.proj(head_outs)\n",
    "\n",
    "        return out\n",
    "            \n",
    "\n",
    "rand = torch.randn(4, 64, 128)\n",
    "attn = MultiHeadAttention(128, 4)\n",
    "attn(rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d2f67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inpur shape :  torch.Size([5, 10]) Output shape :  torch.Size([5, 30])\n",
      "Inpur shape :  torch.Size([5, 1, 2, 4, 10]) Output shape :  torch.Size([5, 1, 2, 4, 30])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(10, 30)\n",
    "\n",
    "tensor_1 = torch.randn(5, 10)\n",
    "tensor_1_out = fc(tensor_1)\n",
    "\n",
    "print(\"Inpur shape : \", tensor_1.shape, \"Output shape : \", tensor_1_out.shape)\n",
    "\n",
    "tensor_2 = torch.randn(5, 1, 2, 4, 10)\n",
    "tensor_2_out = fc(tensor_2)\n",
    "\n",
    "print(\"Inpur shape : \", tensor_2.shape, \"Output shape : \", tensor_2_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f9d07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(1, 8, 9)\n",
    "fc = nn.Linear(9, 9)\n",
    "\n",
    "q = fc(tensor)\n",
    "\n",
    "chunk1, chun2, chunk3 = torch.chunk(q, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b947a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.3403, 0.5602, 0.7023],\n",
       "          [2.3723, 0.7682, 1.0908],\n",
       "          [2.1227, 0.6838, 1.0203],\n",
       "          [1.6837, 0.5906, 0.9572],\n",
       "          [1.6483, 0.5486, 1.1604],\n",
       "          [0.9457, 0.1740, 0.3532]],\n",
       "\n",
       "         [[1.1002, 1.5632, 0.9837],\n",
       "          [0.9615, 1.4630, 1.0461],\n",
       "          [1.0916, 1.3000, 0.6108],\n",
       "          [0.4304, 0.7281, 0.4745],\n",
       "          [0.6627, 0.9203, 0.4878],\n",
       "          [0.5577, 0.9763, 0.7935]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, 2, 6, 4)\n",
    "b = torch.rand(1, 2, 4, 3)\n",
    "\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155ed6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 128])\n",
      "torch.Size([4, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p = 0.0, proj_p = 0.0, bias = True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, embed_dim = x.shape\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        q = self.query(x).reshape(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).reshape(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).reshape(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = q @ k.transpose(-2, -1) * self.head_dim ** 0.5\n",
    "        attn = attn.softmax(axis=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(batch_size, sequence_length, embed_dim)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "\n",
    "rand = torch.randn(4, 16, 128)\n",
    "attn = SelfAttentionEncoder(128, 4)\n",
    "attn(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e0f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6]) torch.Size([1, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), torch.Size([1, 6, 6]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_attn = torch.rand(1, 6, 6)\n",
    "\n",
    "attention_mask = torch.tensor([1, 1, 1, 1, 0, 0]).unsqueeze(0).bool()\n",
    "print(attention_mask.shape)\n",
    "# attention_mask = attention_mask.unsqueeze(1)\n",
    "print(attention_mask.shape, rand_attn.shape)\n",
    "\n",
    "# torch.softmax(rand_attn.masked_fill(~attention_mask, -float(\"inf\")), axis=-1)\n",
    "\n",
    "attention_mask.shape, rand_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa693c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
